{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN-ZzSPFEQRg"
   },
   "source": [
    "# Group Assignment 1: Human Activity Detection\n",
    "In this assignment you will create you own dataset for classification. You will explore which ML algorithms are best to classify this and you will present your best solution. \n",
    "\n",
    "- Create your own dataset for custom human motions using Phyphox\n",
    "- There should be at least 3 distinct types of motions\n",
    "- The motions should be differentÂ to the ones used in the UCI dataset (Not: walking, sitting, standing, laying, stairs)\n",
    "- Follow the steps and answer the questions given in this notebook\n",
    "\n",
    "### Generating your dataset:\n",
    "\n",
    "For this assignment you will create your own dataset of motions that you collect with an Accelerometer and Gyroscope. For this you can use your phone as a sensor.\n",
    "To be able to collect your data you can best use an app called [phyphox](https://phyphox.org/), this is a free app available in app stores. This app can be configured to acces your sensordata, sample it as given frequency's. you can set it up te have experiment timeslots, and the data with a timestamp can be exported to a needed output format.\n",
    "\n",
    "![](https://phyphox.org/wp-content/uploads/2019/06/phyphox_dark-1024x274.png)\n",
    "\n",
    "When you installed the app you can setup a custum experiment by clicking on the + button. Define an experiment name, sample frequency and activate the Accelerometer and Gyroscope. Your custom experiment will be added, you can run it pressing the play button and you will see sensor motion. Pressing the tree dots (...) lets you define timed runs, remote access and exporting data.\n",
    "\n",
    "Phyphox will generate 2 files with sensor data, one for the Accelerometer and one for the Giro. Both files will have timestamps which might not match the recorded sensor data for each sensor. Please, preprocess and merge the files for using it as your dataset for training, testing and deploying your own supervised learning model.\n",
    "\n",
    "### steps\n",
    "\n",
    "With your own generated dataset the similar sequence of steps should be taken to train your model.\n",
    "\n",
    "These are the generic steps to be taken\n",
    "1. Frame the problem and look at the big picture.\n",
    "2. Get the data.\n",
    "3. Explore the data to gain insights.\n",
    "4. Prepare the data to better expose the underlying data patterns to Machine Learning algorithms\n",
    "5. Explore many different models and short-list the best ones.\n",
    "6. Fine-tune your models and combine them into a great solution.\n",
    "7. Present your solution.\n",
    "8. Launch, monitor, and maintain your system.\n",
    "9. Additional Questions\n",
    "\n",
    "\n",
    "---\n",
    "In the Notebook this structure is used for dividing the different steps, so make sure you do the implementation and analisis at these location in the notebook. \n",
    "\n",
    "You may add additinal code blocks, but keep the seperation of the given structure.\n",
    "\n",
    "At the end of each block summarize / comment / conclude your current step in the given textblocks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVvVxhvpne6O"
   },
   "source": [
    "```\n",
    "Roel van der Leest // 4910087\n",
    "Jari van hoof // 4938135\n",
    "Wout van der zanden // 4845250\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVhu3sWzEQRv"
   },
   "source": [
    "# 1. Frame the problem and look at the big picture\n",
    "*Describe the problem at hand and explain your approach*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDdDUEHGgRe_"
   },
   "source": [
    "### Problem Description\n",
    "The goal of this assignment is to develop a machine learning model capable of recognizing specific human activities based on sensor data. We will distinguish between three gestures using data collected from a smartphone's accelerometer and gyroscope.\n",
    "\n",
    "### Selected Activities\n",
    "We have chosen three motions that involve different arm trajectories and intensities:\n",
    "1.  **Drawing the letter 'O':** A continuous circular motion performed in the air.\n",
    "2.  **Throwing a ball:** A rapid, linear acceleration followed by a deceleration.\n",
    "3.  **Opening a door:** A reach-and-pull or push-and-turn motion.\n",
    "\n",
    "### Approach & Methodology\n",
    "*   **Data Collection:** We will use the **Phyphox** app to record 3-axis Accelerometer and Gyroscope data.\n",
    "*   **Hardware Consistency:** To eliminate overfitting to one specific device its sensors, we will use 2 different smartphones for all data collection. If this turns out to still be an issue, we will add 1 device more.\n",
    "*   **Bias Mitigation:** To ensure our model learns the motion rather than a specific person's movement style, we will have each group member perform 10 movements with one phone, following 10 times the same movement with the other phone. this results in total 60 samples per activity which consist of 2 different phones and 3 different people, totalling 180 samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkPzAWZ6d2PY"
   },
   "source": [
    "# 2. Get the data.\n",
    "\n",
    "### Data Collection Protocol\n",
    "To ensure good data and minimize bias, we have established the following protocol:\n",
    "*   **Device:** Two phones will be used for all recordings to eliminate hardware sensor variance.\n",
    "*   **Participants:** each group member will perform the motions to prevent the model from overfitting to a single person's movement style.\n",
    "*   **Volume:** We aim for 20 samples per person per movement with 10 samples on each phone.\n",
    "*   **Sampling:** We will use fixed time windows in Phyphox to ensure consistent data length for each sample.\n",
    "\n",
    "### The Classes\n",
    "We will classify the following 3 activities:\n",
    "\n",
    "#### 1. Opening a Door\n",
    "*   **Action:** Miming the action of reaching for a handle, turning/pushing, and returning.\n",
    "*   **Start Position:** Hands beside the body.\n",
    "*   **Movement:** As shown in the video [TODO: Add video reference].\n",
    "*   **End Position:** Hands beside the body.\n",
    "*   **Phone Orientation:** [TODO: Add image of phone grip]\n",
    "\n",
    "#### 2. Drawing the Letter 'O'\n",
    "*   **Action:** Drawing a large circle in the air.\n",
    "*   **Start Position:** Hands in front of the body at the top of the 'O'.\n",
    "*   **Movement:** As shown in the video [TODO: Add video reference].\n",
    "*   **End Position:** Same as start position (completing the loop).\n",
    "*   **Phone Orientation:** [TODO: Add image of phone grip]\n",
    "\n",
    "#### 3. Throwing a Ball\n",
    "*   **Action:** Miming an overhand throw.\n",
    "*   **Start Position:** Hands beside the body.\n",
    "*   **Movement:** As shown in the video [TODO: Add video reference].\n",
    "*   **End Position:** Hand at the top of the arc, after the \"release\".\n",
    "*   **Phone Orientation:** [TODO: Add image of phone grip]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phXQcbhjjc8E"
   },
   "source": [
    "<!-- ### Data Loading & Merging Strategy\n",
    "\n",
    "**Addressing your question:** *\"Shouldn't we separate by folder?\"*\n",
    "\n",
    "We **are** separating the data logically, but storing it in a single efficient structure for Machine Learning.\n",
    "\n",
    "**1. The `Activity` Label:**\n",
    "Instead of creating separate variables for each folder, we add an `Activity` column.\n",
    "*   The script automatically detects if a sample is \"Door Movement\", \"Letter O\", or \"Throwing Ball\" based on the folder path.\n",
    "*   This creates the **Labels ($y$)** required for supervised learning.\n",
    "\n",
    "**2. The `Sample_ID`:**\n",
    "*   We assign a unique `Sample_ID` to every 5-second recording found.\n",
    "*   This allows us to distinguish between individual movements (e.g., \"Door Movement #1\", \"Door Movement #2\") within the large dataset.\n",
    "\n",
    "**Why this format?**\n",
    "This \"Long Format\" (Time-Series) is the standard starting point. In **Step 4**, we will group by `Sample_ID` to calculate features (e.g., mean acceleration, max gyro) for each unique movement, creating the final training set. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0v8cn-1reCYH"
   },
   "source": [
    "# 3. Explore the data to gain insights.\n",
    "\n",
    "*Explore the data in any possible way, visualize the results (if you have multiple plots of the same kind of data put them in one larger plot)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After conducting all the data collection with pyphox all the data was inside a Zip file, plus it included unnecessary meta data. Manually unzipping and cleaning this data would be very time consuming. Therefore, we used a python script that automatically unzips all files and removes unnecessary meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Door movement data\n",
      "Processing: letter O movement data\n",
      "Processing: throwing ball movement data\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "def unzip_and_clean(target_directory):\n",
    "    \n",
    "    if not os.path.exists(target_directory):\n",
    "        print(f\"Directory not found: {target_directory}\")\n",
    "        return\n",
    "\n",
    "    # Walk through all directories recursively\n",
    "    for root, dirs, files in os.walk(target_directory):\n",
    "        # Check all files in the current directory\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "\n",
    "            # Check if it's a zip file\n",
    "            if filename.lower().endswith('.zip'):\n",
    "                # Create a directory name based on the zip file (removing .zip extension)\n",
    "                extract_folder_name = os.path.splitext(filename)[0]\n",
    "                extract_path = os.path.join(root, extract_folder_name)\n",
    "                \n",
    "                # Unzip\n",
    "                try:\n",
    "                    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                        zip_ref.extractall(extract_path)\n",
    "                    \n",
    "                    # Remove 'meta' directory if it exists\n",
    "                    meta_dir = os.path.join(extract_path, 'meta')\n",
    "                    if os.path.exists(meta_dir) and os.path.isdir(meta_dir):\n",
    "                        shutil.rmtree(meta_dir)\n",
    "                        \n",
    "                    os.remove(file_path)\n",
    "                except zipfile.BadZipFile:\n",
    "                    print(f\"Error: {filename} is a bad zip file.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Base directory containing all movement data folders\n",
    "    base_dir = r\"C:\\Users\\roelv\\Documents\\Machine Learning\\AIS\\assignment 1\"\n",
    "    \n",
    "    # Process all movement data directories\n",
    "    movement_types = [\"Door movement data\", \"letter O movement data\", \"throwing ball movement data\"]\n",
    "    \n",
    "    for movement_type in movement_types:\n",
    "        target_dir = os.path.join(base_dir, movement_type)\n",
    "        if os.path.exists(target_dir):\n",
    "            print(f\"Processing: {movement_type}\")\n",
    "            unzip_and_clean(target_dir)\n",
    "    \n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After succesfully unzipping and cleaning the data, we load all the data files into a single pandas DataFrame. Each file is labeled with its corresponding activity based on the folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 45096 data points.\n",
      "Total unique samples (recordings): 180\n",
      "\n",
      "Class distribution:\n",
      "Activity\n",
      "Throwing Ball    15052\n",
      "Letter O         15048\n",
      "Door Movement    14996\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Acc_X</th>\n",
       "      <th>Acc_Y</th>\n",
       "      <th>Acc_Z</th>\n",
       "      <th>Gyro_X</th>\n",
       "      <th>Gyro_Y</th>\n",
       "      <th>Gyro_Z</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Sample_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001773</td>\n",
       "      <td>9.166488</td>\n",
       "      <td>-3.286116</td>\n",
       "      <td>2.056723</td>\n",
       "      <td>0.310439</td>\n",
       "      <td>-0.062838</td>\n",
       "      <td>0.070347</td>\n",
       "      <td>Door Movement</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.018317</td>\n",
       "      <td>9.422306</td>\n",
       "      <td>-3.262615</td>\n",
       "      <td>2.197879</td>\n",
       "      <td>0.362224</td>\n",
       "      <td>-0.108883</td>\n",
       "      <td>0.053720</td>\n",
       "      <td>Door Movement</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.038407</td>\n",
       "      <td>9.295220</td>\n",
       "      <td>-3.129093</td>\n",
       "      <td>1.997147</td>\n",
       "      <td>0.347363</td>\n",
       "      <td>-0.165629</td>\n",
       "      <td>0.053675</td>\n",
       "      <td>Door Movement</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058497</td>\n",
       "      <td>9.314381</td>\n",
       "      <td>-3.121459</td>\n",
       "      <td>2.042054</td>\n",
       "      <td>0.297007</td>\n",
       "      <td>-0.155002</td>\n",
       "      <td>0.038126</td>\n",
       "      <td>Door Movement</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.078587</td>\n",
       "      <td>9.304651</td>\n",
       "      <td>-3.115471</td>\n",
       "      <td>2.310296</td>\n",
       "      <td>0.324873</td>\n",
       "      <td>-0.132150</td>\n",
       "      <td>0.095936</td>\n",
       "      <td>Door Movement</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time     Acc_X     Acc_Y     Acc_Z    Gyro_X    Gyro_Y    Gyro_Z  \\\n",
       "0 -0.001773  9.166488 -3.286116  2.056723  0.310439 -0.062838  0.070347   \n",
       "1  0.018317  9.422306 -3.262615  2.197879  0.362224 -0.108883  0.053720   \n",
       "2  0.038407  9.295220 -3.129093  1.997147  0.347363 -0.165629  0.053675   \n",
       "3  0.058497  9.314381 -3.121459  2.042054  0.297007 -0.155002  0.038126   \n",
       "4  0.078587  9.304651 -3.115471  2.310296  0.324873 -0.132150  0.095936   \n",
       "\n",
       "        Activity  Sample_ID  \n",
       "0  Door Movement          1  \n",
       "1  Door Movement          1  \n",
       "2  Door Movement          1  \n",
       "3  Door Movement          1  \n",
       "4  Door Movement          1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_label_from_path(path):\n",
    "    path_lower = path.lower()\n",
    "    if 'door' in path_lower:\n",
    "        return 'Door Movement'\n",
    "    elif 'letter o' in path_lower:\n",
    "        return 'Letter O'\n",
    "    elif 'ball' in path_lower or 'throw' in path_lower:\n",
    "        return 'Throwing Ball'\n",
    "    return 'Unknown'\n",
    "\n",
    "def load_and_merge_data(base_dir):\n",
    "    all_data = []\n",
    "    sample_id_counter = 1\n",
    "    \n",
    "    # Walk through all folders\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        # We only care about folders that contain both sensor files\n",
    "        if 'Accelerometer.csv' in files and 'Gyroscope.csv' in files:\n",
    "            try:\n",
    "                # 1. Construct file paths\n",
    "                acc_path = os.path.join(root, 'Accelerometer.csv')\n",
    "                gyro_path = os.path.join(root, 'Gyroscope.csv')\n",
    "                \n",
    "                # 2. Load the data\n",
    "                df_acc = pd.read_csv(acc_path)\n",
    "                df_gyro = pd.read_csv(gyro_path)\n",
    "                \n",
    "                # 3. Rename columns to avoid confusion after merge\n",
    "                # renaming for clarity\n",
    "                df_acc = df_acc.rename(columns={\n",
    "                    df_acc.columns[0]: 'Time', \n",
    "                    df_acc.columns[1]: 'Acc_X', \n",
    "                    df_acc.columns[2]: 'Acc_Y', \n",
    "                    df_acc.columns[3]: 'Acc_Z'\n",
    "                })\n",
    "                \n",
    "                df_gyro = df_gyro.rename(columns={\n",
    "                    df_gyro.columns[0]: 'Time', \n",
    "                    df_gyro.columns[1]: 'Gyro_X', \n",
    "                    df_gyro.columns[2]: 'Gyro_Y', \n",
    "                    df_gyro.columns[3]: 'Gyro_Z'\n",
    "                })\n",
    "                \n",
    "                # 4. Sort by time (required for merge_asof)\n",
    "                df_acc = df_acc.sort_values('Time')\n",
    "                df_gyro = df_gyro.sort_values('Time')\n",
    "                \n",
    "                # 5. Merge: Align Gyro data to the nearest Accelerometer timestamp\n",
    "                # This handles the mismatched time frames/frequencies\n",
    "                df_merged = pd.merge_asof(df_acc, df_gyro, on='Time', direction='nearest')\n",
    "                \n",
    "                # 6. Add Metadata\n",
    "                df_merged['Activity'] = get_label_from_path(root)\n",
    "                df_merged['Sample_ID'] = sample_id_counter\n",
    "                \n",
    "                all_data.append(df_merged)\n",
    "                sample_id_counter += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {root}: {e}\")\n",
    "                \n",
    "    # Combine all samples into one big DataFrame\n",
    "    if all_data:\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Run the loading function\n",
    "# We use '.' to search in the current directory and subdirectories\n",
    "df_final = load_and_merge_data('.')\n",
    "\n",
    "# Display results\n",
    "print(f\"Successfully loaded {len(df_final)} data points.\")\n",
    "print(f\"Total unique samples (recordings): {df_final['Sample_ID'].nunique()}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df_final['Activity'].value_counts())\n",
    "\n",
    "# Show the first few rows\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Single DataFrame (long format):** We combine all recordings into one DataFrame so analysis, plotting, filtering, and later feature extraction are simple and consistent. A single table lets us easily group by recording Sample_ID and activity.\n",
    "\n",
    "- **`Sample_ID`:** Each folder contains one recording (one physical gesture). We assign a unique `Sample_ID` to every recording so we can treat each gesture as one sample later. During feature extraction we will group by Sample_ID\n",
    "\n",
    "- **Merging accelerometer and gyroscope:** Phyphox exports two separate files per recording (Accelerometer.csv and Gyroscope.csv) with their own timestamps and sampling jitter. We use `pd.merge_asof` to align them: it matches each accelerometer timestamp with the nearest gyroscope timestamp. This is preferred because:\n",
    "  - It preserves the main timeline (we merge onto `Time` from one sensor).\n",
    "  - It deals robustly with slightly different sampling rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5kdsQwneWZ7"
   },
   "source": [
    "# 4. Prepare the data to better expose the underlying data patterns to Machine Learning algorithms\n",
    "\n",
    "prepare your data, is it normalized? are there outlier? Make a training and a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY_GYjQSfWnF"
   },
   "source": [
    "# 5. Explore many different models and short-list the best ones.\n",
    "\n",
    "Explore / train and list the top 3 algorithms that score best on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAz7LbB2feBt"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwv3xz1wjows"
   },
   "source": [
    "```\n",
    "# Place your comments / conclusions / insight here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIL2-EUJffYV"
   },
   "source": [
    "# 6. Fine-tune your models and combine them into a great solution.\n",
    "\n",
    "can you get better performance within a model? e.g if you use a KNN classifier how does it behave if you change K (k=3 vs k=5 vs k=?). Which parameters are here to tune in the chosen models? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bG5g80NfpMk"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvWBlVKeklAE"
   },
   "source": [
    "```\n",
    "# Place your comments / conclusions / insight here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvnEhvXwfrlm"
   },
   "source": [
    "# 7. Present your solution.\n",
    "\n",
    "Explain why you would choose for a specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6NTnFVCfww-"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82zdBc53kmb4"
   },
   "source": [
    "```\n",
    "# Place your comments / conclusions / insight here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1hc2Ffofz79"
   },
   "source": [
    "# 8. Launch, monitor, and maintain your system.\n",
    "\n",
    "Can you Deployment the model?\n",
    "\n",
    "> NOTE: The app provides the option for remote access, so you are able to get live sensordata from the phone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJ1uQFlood1G"
   },
   "source": [
    "# 9. Additional Questions\n",
    "\n",
    "* Explain the chosen motions you chose to be classified. \n",
    "\n",
    "* Which of these motions is easier/harder to classify and why?\n",
    "\n",
    "* After your experience, which extra sensor data might help getting a better classifier and why?\n",
    "\n",
    "* Explain why you think that your chosen algorithm outperforms the rest? \n",
    "\n",
    "* While recording the same motions with the same sensor data, what do you think will help improving the performance of your models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vi5EGcnIoiyL"
   },
   "source": [
    "```\n",
    "# Place your comments / conclusions / insight here\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "comparing-classifiers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
