{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN-ZzSPFEQRg"
   },
   "source": [
    "# Group Assignment 1: Human Activity Detection\n",
    "In this assignment you will create you own dataset for classification. You will explore which ML algorithms are best to classify this and you will present your best solution. \n",
    "\n",
    "- Create your own dataset for custom human motions using Phyphox\n",
    "- There should be at least 3 distinct types of motions\n",
    "- The motions should be differentÂ to the ones used in the UCI dataset (Not: walking, sitting, standing, laying, stairs)\n",
    "- Follow the steps and answer the questions given in this notebook\n",
    "\n",
    "### Generating your dataset:\n",
    "\n",
    "For this assignment you will create your own dataset of motions that you collect with an Accelerometer and Gyroscope. For this you can use your phone as a sensor.\n",
    "To be able to collect your data you can best use an app called [phyphox](https://phyphox.org/), this is a free app available in app stores. This app can be configured to acces your sensordata, sample it as given frequency's. you can set it up te have experiment timeslots, and the data with a timestamp can be exported to a needed output format.\n",
    "\n",
    "![](https://phyphox.org/wp-content/uploads/2019/06/phyphox_dark-1024x274.png)\n",
    "\n",
    "When you installed the app you can setup a custum experiment by clicking on the + button. Define an experiment name, sample frequency and activate the Accelerometer and Gyroscope. Your custom experiment will be added, you can run it pressing the play button and you will see sensor motion. Pressing the tree dots (...) lets you define timed runs, remote access and exporting data.\n",
    "\n",
    "Phyphox will generate 2 files with sensor data, one for the Accelerometer and one for the Giro. Both files will have timestamps which might not match the recorded sensor data for each sensor. Please, preprocess and merge the files for using it as your dataset for training, testing and deploying your own supervised learning model.\n",
    "\n",
    "### steps\n",
    "\n",
    "With your own generated dataset the similar sequence of steps should be taken to train your model.\n",
    "\n",
    "These are the generic steps to be taken\n",
    "1. Frame the problem and look at the big picture.\n",
    "2. Get the data.\n",
    "3. Explore the data to gain insights.\n",
    "4. Prepare the data to better expose the underlying data patterns to Machine Learning algorithms\n",
    "5. Explore many different models and short-list the best ones.\n",
    "6. Fine-tune your models and combine them into a great solution.\n",
    "7. Present your solution.\n",
    "8. Launch, monitor, and maintain your system.\n",
    "9. Additional Questions\n",
    "\n",
    "\n",
    "---\n",
    "In the Notebook this structure is used for dividing the different steps, so make sure you do the implementation and analisis at these location in the notebook. \n",
    "\n",
    "You may add additinal code blocks, but keep the seperation of the given structure.\n",
    "\n",
    "At the end of each block summarize / comment / conclude your current step in the given textblocks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVvVxhvpne6O"
   },
   "source": [
    "```\n",
    "Roel van der Leest // 4910087\n",
    "Jari van hoof // 4938135\n",
    "Wout van der zanden // 4845250\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVhu3sWzEQRv"
   },
   "source": [
    "# 1. Frame the problem and look at the big picture\n",
    "*Describe the problem at hand and explain your approach*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDdDUEHGgRe_"
   },
   "source": [
    "### Problem Description\n",
    "The goal of this assignment is to develop a machine learning model capable of recognizing specific human activities based on sensor data. We will distinguish between three gestures using data collected from a smartphone's accelerometer and gyroscope.\n",
    "\n",
    "### Selected Activities\n",
    "We have chosen three motions that involve different arm trajectories and intensities:\n",
    "1.  **Drawing the letter 'O':** A continuous circular motion performed in the air.\n",
    "2.  **Throwing a ball:** A rapid, linear acceleration followed by a deceleration.\n",
    "3.  **Opening a door:** A reach-and-pull or push-and-turn motion.\n",
    "\n",
    "### Approach & Methodology\n",
    "*   **Data Collection:** We will use the **Phyphox** app to record 3-axis Accelerometer and Gyroscope data.\n",
    "*   **Hardware Consistency:** To eliminate noise introduced by different sensors or calibration issues, we will use a single smartphone for all data collection.\n",
    "*   **Bias Mitigation:** To ensure our model learns the *motion* rather than a specific person's movement style, we will have different persons perform each activity. This helps in creating a more robust model that generalizes better to new users.\n",
    "*   **Pipeline:** The collected data will be preprocessed (noise filtering, normalization), feature engineered (extracting relevant statistical features), and used to train the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkPzAWZ6d2PY"
   },
   "source": [
    "# 2. Get the data.\n",
    "\n",
    "### Data Collection Protocol\n",
    "To ensure good data and minimize bias, we have established the following protocol:\n",
    "*   **Device:** A single smartphone will be used for all recordings to eliminate hardware sensor variance.\n",
    "*   **Participants:** each group member will perform the motions to prevent the model from overfitting to a single person's movement style.\n",
    "*   **Volume:** We aim for 30 samples per person per movement.\n",
    "*   **Sampling:** We will use fixed time windows in Phyphox to ensure consistent data length for each sample.\n",
    "\n",
    "### The Classes\n",
    "We will classify the following 3 activities:\n",
    "\n",
    "#### 1. Opening a Door\n",
    "*   **Action:** Miming the action of reaching for a handle, turning/pushing, and returning.\n",
    "*   **Start Position:** Hands beside the body.\n",
    "*   **Movement:** As shown in the video [TODO: Add video reference].\n",
    "*   **End Position:** Hands beside the body.\n",
    "*   **Phone Orientation:** [TODO: Add image of phone grip]\n",
    "\n",
    "#### 2. Drawing the Letter 'O'\n",
    "*   **Action:** Drawing a large circle in the air.\n",
    "*   **Start Position:** Hands in front of the body at the top of the 'O'.\n",
    "*   **Movement:** As shown in the video [TODO: Add video reference].\n",
    "*   **End Position:** Same as start position (completing the loop).\n",
    "*   **Phone Orientation:** [TODO: Add image of phone grip]\n",
    "\n",
    "#### 3. Throwing a Ball\n",
    "*   **Action:** Miming an overhand throw.\n",
    "*   **Start Position:** Hands beside the body.\n",
    "*   **Movement:** As shown in the video [TODO: Add video reference].\n",
    "*   **End Position:** Hand at the top of the arc, after the \"release\".\n",
    "*   **Phone Orientation:** [TODO: Add image of phone grip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zP8ihRoAd_oY"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phXQcbhjjc8E"
   },
   "source": [
    "```\n",
    "# Place your comments / conclusions / insight here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0v8cn-1reCYH"
   },
   "source": [
    "# 3. Explore the data to gain insights.\n",
    "\n",
    "Explore the data in any possible way, visualize the results (if you have multiple plots of the same kind of data put them in one larger plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlhsbvzBeG4E"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZs7EkHfjhT9"
   },
   "source": [
    "```\n",
    "# Place your comments / conclusions / insight here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5kdsQwneWZ7"
   },
   "source": [
    "# 4. Prepare the data to better expose the underlying data patterns to Machine Learning algorithms\n",
    "\n",
    "prepare your data, is it normalized? are there outlier? Make a training and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0yqpHJUfUa5"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nz04fD0bjls1"
   },
   "source": [
    "```\n",
    "# Place your comments / conclusions / insight here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY_GYjQSfWnF"
   },
   "source": [
    "# 5. Explore many different models and short-list the best ones.\n",
    "\n",
    "Explore / train and list the top 3 algorithms that score best on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAz7LbB2feBt"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwv3xz1wjows"
   },
   "source": [
    "```\n",
    "# Place your comments / conclusions / insight here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIL2-EUJffYV"
   },
   "source": [
    "# 6. Fine-tune your models and combine them into a great solution.\n",
    "\n",
    "can you get better performance within a model? e.g if you use a KNN classifier how does it behave if you change K (k=3 vs k=5 vs k=?). Which parameters are here to tune in the chosen models? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bG5g80NfpMk"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvWBlVKeklAE"
   },
   "source": [
    "```\n",
    "# Place your comments / conclusions / insight here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvnEhvXwfrlm"
   },
   "source": [
    "# 7. Present your solution.\n",
    "\n",
    "Explain why you would choose for a specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6NTnFVCfww-"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82zdBc53kmb4"
   },
   "source": [
    "```\n",
    "# Place your comments / conclusions / insight here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1hc2Ffofz79"
   },
   "source": [
    "# 8. Launch, monitor, and maintain your system.\n",
    "\n",
    "Can you Deployment the model?\n",
    "\n",
    "> NOTE: The app provides the option for remote access, so you are able to get live sensordata from the phone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJ1uQFlood1G"
   },
   "source": [
    "# 9. Additional Questions\n",
    "\n",
    "* Explain the chosen motions you chose to be classified. \n",
    "\n",
    "* Which of these motions is easier/harder to classify and why?\n",
    "\n",
    "* After your experience, which extra sensor data might help getting a better classifier and why?\n",
    "\n",
    "* Explain why you think that your chosen algorithm outperforms the rest? \n",
    "\n",
    "* While recording the same motions with the same sensor data, what do you think will help improving the performance of your models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vi5EGcnIoiyL"
   },
   "source": [
    "```\n",
    "# Place your comments / conclusions / insight here\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "comparing-classifiers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
